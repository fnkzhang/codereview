import json, re
from typing import List, Optional
from textwrap import dedent

# pip install "google-cloud-aiplatform>=1.38"
# pip install --upgrade google-cloud-aiplatform
# https://ai.google.dev/docs/prompt_best_practices
import vertexai
from vertexai.generative_models import (
    GenerativeModel,
    Content, 
    Part
)


#------------------------------------------------------------------------------
# Initialize LLM
#-------------------------------------------------------------------------------
MODEL_NAME = "gemini-1.5-pro-preview-0409"
def init_llm(project_id: str="codereview-413200",
             location: str="us-central1"):
    vertexai.init(project=project_id, location=location)

#------------------------------------------------------------------------------
# Helper Functions
#-------------------------------------------------------------------------------
def get_content(role: str,
                text: str):
    return Content(role=role,
                   parts=[Part.from_text(text)])

def get_json_from_llm_response(response: str):
    json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)

    json_response = json_match.group(1) if json_match else response
    print("JSON response by LLM: ", json_response)

    return json.loads(json_response)

def get_chat_response(user_prompt: str,
                      system_prompt: str = None,
                      history: Optional[List["Content"]] = None):
    
    model = GenerativeModel(MODEL_NAME,
                            system_instruction=system_prompt)
    chat = model.start_chat(history=history)
    
    response = chat.send_message(user_prompt)
    return response.text

def add_few_shot_prompt(example_input, example_output):
    example = []

    example.append(get_content("user", example_input))
    example.append(get_content("model", example_output))

    return example

#------------------------------------------------------------------------------
# Functions used by route
#-------------------------------------------------------------------------------
def get_llm_code_from_suggestion(code: str,
                                 highlighted_code: str,
                                 start_line: int,
                                 end_line: int,
                                 suggestion: str):
    history = []

    # Configure System Prompt
    system_prompt = dedent("""\
        Apply <suggestion> to <highlighted_code> which is located at lines
        <start_line> to <end_line> of the <code>. Modify the 
        <highlighted_code> only. Avoid modifying <code>. Return the revised
        code in a JSON.\
    """)

    # Provide Few-Shot Examples on how the LLM should respond
    history.extend(
        add_few_shot_prompt(
            example_input = dedent("""\
                <code>: ```def foo():
                            print("Hello World")

                        print("Testing foo()")
                        foo()
                        print("Done testing foo()")```
                <highlighted_code>: ```foo```
                <start_line>: 1
                <end_line>: 1
                <suggestion>: rename to something more meaningful\
            """),
            example_output = dedent("""\
                ```json
                {"revised_code": "say_hello()"}```\
            """)
        )
    )

    # Configure User Prompt
    user_prompt = dedent(f"""\
        <code>: {code}
        <highlighted_code>: {highlighted_code}
        <start_line>: {start_line}
        <end_line>: {end_line}
        <suggestion>: {suggestion}\
    """)

    # Generate a response from LLM
    try:
        response = get_chat_response(user_prompt=user_prompt,
                                     system_prompt=system_prompt,
                                     history=history)
    except Exception as e:
        print("Failed to get response from LLM")
        print(e)
        return None
    
    # Extract the wanted output from response
    try:
        revised_code = get_json_from_llm_response(response)["revised_code"]
    except Exception as e:
        print("Failed to get code from response")
        print(e)
        return None

    print("Implemented Code Generated by AI:", revised_code)
    return revised_code

def get_llm_suggestion_from_code(code: str):
    history = []

    # Configure System Prompt
    system_prompt = dedent("""\
        List some suggestions that would improve the quality of <code> 
        according to best practices. Return the list of suggestions in a 
        JSON that contains other JSONs with the code suggestion, start 
        line of the highlight, and end line of the highlight.\
    """)

    # Provide Few-Shot Examples on how the LLM should respond
    example_suggestions = [
        json.dumps({
            "startLine": 1,
            "endLine": 3,
            "suggestion": "Rename `n`, `tot`, and `cnt` to more descriptive names such as `numbers`, `total_sum`, and `count` respectively."
        }),
        json.dumps({
            "startLine": 4,
            "endLine": 6,
            "suggestion": "This works, but use Python's built-in functions like sum() and len() instead." 
        }),
        json.dumps({
            "startLine": 7,
            "endLine": 7,
            "suggestion": "You forgot a return statement. The function calculates the average but does nothing with it." 
        })
    ]
    example_json = json.dumps({
        "suggestions": example_suggestions
    })

    history.extend(
        add_few_shot_prompt(
            example_input = (
                "<code>: def calc_avg(n):\n    tot=0\n    cnt=0\n    for number in n:\n      tot = tot+ number\n      cnt= cnt+1\n    average=tot/cnt"
            ),
            example_output = (
                "```json\n"
                f"{example_json}\n"
                "```\n"
            )
        )
    )

    # Configure User Prompt
    user_prompt = dedent(f"""\
        <code>: {code}\
    """)

    # Generate a response from LLM
    try:
        response = get_chat_response(user_prompt=user_prompt,
                                     system_prompt=system_prompt,
                                     history=history)
    except Exception as e:
        print("Failed to get response from LLM")
        print(e)
        return None
    
    # Extract the wanted output from response
    try:
        suggestions = get_json_from_llm_response(response)["suggestions"]
    except Exception as e:
        print("Failed to get suggestions from response")
        print(e)
        return None

    print("Comment Suggestions Generated by AI:")
    for suggestion in suggestions:
        print(suggestion, '\n')

    return suggestions
