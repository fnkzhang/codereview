import json, re
from typing import List, Optional

# pip install "google-cloud-aiplatform>=1.38"
# https://ai.google.dev/docs/prompt_best_practices
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig, \
    Content, Part

#------------------------------------------------------------------------------
# Initialize LLM
#-------------------------------------------------------------------------------
model = None
def init_llm(project_id: str="codereview-413200",
             location: str="us-central1"):
    global model
    vertexai.init(project=project_id, location=location)
    model = GenerativeModel("gemini-1.0-pro")

#------------------------------------------------------------------------------
# Helper Functions
#-------------------------------------------------------------------------------
def get_content(role: str,
                text: str):
    return Content(role=role,
                   parts=[Part.from_text(text)])

def get_json_from_llm_response(response: str):
    json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)

    json_response = json_match.group(1) if json_match else response
    print("JSON response by LLM: ", json_response)

    return json.loads(json_response)

def get_chat_response(prompt: str,
                      history: Optional[List["Content"]] = None,
                      temperature: float=0.15):
    text_response = []
    chat = model.start_chat(history=history)
    
    responses = chat.send_message(prompt,
                                  stream=True,
                                  generation_config=GenerationConfig(
                                      temperature=temperature
                                  ))
    for chunk in responses:
        text_response.append(chunk.text)
    return "".join(text_response)

def add_few_shot_prompt(example_input, example_output):
    example = []

    example.append(get_content("user", example_input))
    example.append(get_content("model", example_output))

    return example

#------------------------------------------------------------------------------
# Functions used by route
#-------------------------------------------------------------------------------
def get_llm_code_from_suggestion(code: str,
                                 highlighted_code: str,
                                 suggestion: str):
    history = []

    # Configure System Prompt
    system_prompt = (
        "System Prompt: "
        "Apply [suggestion] to [highlighted_code] which is a substring "
        "located in [code]. Return the revised code as a JSON with the "
        "key being \"revised_code\".\n"
    )

    history.append(get_content("user", system_prompt))
    history.append(get_content("model", "Understood."))

    # Provide Few-Shot Examples on how the LLM should respond
    example_json = json.dumps({
        "revised_code": "say_hello()"
    })
    history.extend(
        add_few_shot_prompt(
            example_input = (
                "[code]: def foo():\n    print(\"Hello World\")\n\nfoo()\n"
                "[highlighted_code]: foo()\n"
                "[suggestion]: rename to something more meaningful\n"
            ),
            example_output = (
                "```json\n"
                f"{example_json}\n"
                "```\n"
            )
        )
    )

    # Configure User Prompt
    user_prompt = (
        f"[code]: {code}\n"
        f"[highlighted_code]: {highlighted_code}\n"
        f"[suggestion]: {suggestion}\n"
    )

    # Generate a response from LLM
    try:
        response = get_chat_response(user_prompt, history=history)
    except:
        print("Failed to get response from LLM")
        return None
    
    # Extract the wanted output from response
    try:
        revised_code = get_json_from_llm_response(response)["revised_code"]
    except:
        print("Failed to get code from response")
        return None

    print("Implemented Code Generated by AI:", revised_code)
    return revised_code

def get_llm_suggestion_from_code(code: str):
    history = []

    # Configure System Prompt
    system_prompt = (
        "System Prompt: "
        "List some suggestions that would improve the quality of [code] "
        "according to best practices. Return the list of suggestions as "
        "a JSON with the key being \"suggestions\". with the value being "
        "a JSON. A JSON suggestion should contain the keys \"startLine\" "
        "and \"endLine\" to highlight the section of code being referenced "
        "and the key \"suggestion\" to store the suggestion.\n"
    )

    history.append(get_content("user", system_prompt))
    history.append(get_content("model", "Understood."))

    # Provide Few-Shot Examples on how the LLM should respond
    example_suggestions = [
        json.dumps({
            "startLine": 1,
            "endLine": 3,
            "suggestion": "Rename `n`, `tot`, and `cnt` to more descriptive names such as `numbers`, `total_sum`, and `count` respectively."
        }),
        json.dumps({
            "startLine": 4,
            "endLine": 6,
            "suggestion": "This works, but use Python's built-in functions like sum() and len() instead." 
        }),
        json.dumps({
            "startLine": 7,
            "endLine": 7,
            "suggestion": "You forgot a return statement. The function calculates the average but does nothing with it." 
        })
    ]
    example_json = json.dumps({
        "suggestions": example_suggestions
    })

    history.extend(
        add_few_shot_prompt(
            example_input = (
                "[code]: def calc_avg(n):\n    tot=0\n    cnt=0\n    for number in n:\n      tot = tot+ number\n      cnt= cnt+1\n    average=tot/cnt"
            ),
            example_output = (
                "```json\n"
                f"{example_json}\n"
                "```\n"
            )
        )
    )

    # Configure User Prompt
    user_prompt = (
        f"[code]: {code}\n"
    )

    # Generate a response from LLM
    try:
        response = get_chat_response(user_prompt, history=history)
    except:
        print("Failed to get response from LLM")
        return None
    
    # Extract the wanted output from response
    try:
        suggestions = get_json_from_llm_response(response)["suggestions"]
    except:
        print("Failed to get suggestions from response")
        return None

    print("Comment Suggestions Generated by AI:")
    for suggestion in suggestions:
        print(suggestion, '\n')

    return suggestions
